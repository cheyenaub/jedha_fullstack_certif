# Data Projects Repository for Jedha certification

Jedha Fullstack : Certification projects

1. Build and manage a Data Infrastructure

Kayak
Extracted hotels and weather data for specified cities through web scraping
Processed and stored clean data in an AWS S3 bucket, subsequently transferring it to an AWS RDS database instance
Executed SQL queries on the database to identify optimal cities based on weather conditions
Generated maps displaying selected city and hotel locations with daily temperature and hotel price range information
Technologies: APIs, Scrapy, Boto3, AWS S3, AWS RDS, SQL, SQLAlchemy, Plotly

2. Exploratory Data Analysis (EDA)

Speed Dating Dataset
Analyzed a speed dating dataset to identify key factors influencing the capacity of getting a date
Technologies: Pandas, Numpy, Seaborn, Matplotlib, Plotly

3. Predictive Analysis of Structured Data Using Artificial Intelligence

Conversion Challenge
Developed machine learning models to predict newsletter conversion rates using imbalanced features  

Walmart
Estimated weekly sales in stores by constructing machine learning models  

Uber
Created algorithms to optimize driver wait times by identifying high-demand zones during specific times of the day
Technologies: Pandas, Seaborn, Matplotlib, Scikit-learn

4. Predictive Analysis of Unstructured Data Using Deep Learning

AT&T Spam Detector
Developed a deep learning model to automatically detect spam messages based on their content
Technologies: TensorFlow, Keras, Transformers, Natural Language Processing (Tokenization, Encoding, Lemmatization, Embedding)

5. Industrialization of Machine Learning Algorithms and Automation of Decision-Making Processes

Getaround
Designed a dashboard for a business team to optimize rental pricing by determining the optimal rental delay
Created an online API with a prediction endpoint for rental pricing
Technologies: Streamlit, Docker, FastAPI, Machine Learning, Heroku, GitHub

6. Data Management Project Administration

DataMatch (Team Project with Fran√ßois and Pauline)
Simplify job search process in the field of data by creating clusters which provide job offers based on skills (software, languages, etc.) and not job titles.
This will entail employing text processing techniques and clustering methods to categorize job descriptions based on their skill, software, and programming language requirements.

Data will be sourced from various job posting websites using web scraping tools, followed by rigorous data cleansing before modeling begins.
Furthermore, an interactive Streamlit application will be created to offer a comprehensive view of the data job market, elucidate the clusters, and explore job listings that have undergone processing by the model.
Technologies: Pandas,Seaborn, Scrapy, Plotly, Streamlit, Tensorflow
